#  大量用户，我要存储用户和用户的好友关系，用什么数据结构来设计存储？

这是一个典型的“**好友关系建模**”问题，结合“**大量用户**”和“**图论**”的提示，我们可以从**图的角度建模用户关系网络**。

## 核心思路

社交网络、关系推荐、路径查找等复杂图查询 —— 用图数据库天然契合。

将“用户”建模为**图的节点（Vertex）**，将“好友关系”建模为**图中的边（Edge）**：

- 如果好友关系是**单向的**（如关注），用**有向图（Directed Graph）**
- 如果好友关系是**双向的**（如微信好友），用**无向图（Undirected Graph）**



对于“**大量用户**”的高效存储和查询，使用**邻接表**构建图

```java
// Java 中的结构：Map<用户ID, Set<好友ID>>
Map<Integer, Set<Integer>> userGraph = new HashMap<>();
```

优点：

- 占用空间小（O(N + E)）
- 查询某个用户的所有好友效率高（O(k)，k为好友数）

[如果要快速查找**两个用户是否为好友**， 可以考虑使用 **邻接矩阵**]



## Redis 实现（大厂常用方案）

Redis 更适合做**高性能好友关系缓存**，适合读多写少的查询场景

```java
SADD friends:123 456 789
SADD friends:456 123
```

查询操作

```java
SMEMBERS friends:123
```

查询共同好友

```java
SINTER friends:123 friends:456
```



## 常见查询场景及算法实现

| **查询场景**     | **描述**                      | **实现方法（Java）**                        |
| ---------------- | ----------------------------- | ------------------------------------------- |
| 查询某个用户好友 | 用户所有直接连接的用户        | Map<Integer, Set> 中查找即可                |
| 查询共同好友     | A 和 B 的好友集合取交集       | SetUtils.intersection(setA, setB)           |
| 是否路径可达     | A 和 B 是否能通过若干好友连接 | DFS / BFS                                   |
| 推荐好友         | 查找“朋友的朋友”但非直接好友  | BFS 深度为2 + 排除自己和直接好友 + 计数排序 |
| 社区划分         | 找出紧密相连的用户群体        | 图的聚类算法：Louvain、Label Propagation 等 |



# 多个文件上传且文件名完全相同，如何保证线程安全？

这是一个**并发场景下的文件上传冲突问题**，重点考察**线程安全 + 文件重名冲突处理策略**，常出现在高并发系统、网盘、OSS等场景。



## 问题本质

多个用户或多个线程**几乎同时上传相同文件名的文件**，如果你用文件名作为唯一标识，可能导致：

- 文件被覆盖（线程不安全）
- 文件上传失败（命名冲突）
- 数据错乱（写入时未加锁）



## 方案1：**文件名做唯一化（加 UUID 或时间戳）**

在上传之前，**重命名文件**，使得文件名具有全局唯一性。

```java
String originalFileName = "avatar.png";
String suffix = originalFileName.substring(originalFileName.lastIndexOf("."));
String newFileName = UUID.randomUUID().toString() + suffix;
// 最终保存路径如：/upload/2025/04/11/2f4e-xxx.png
```

## 方案2：**使用用户私有目录隔离**

```java
/upload/user123/avatar.png
/upload/user456/avatar.png
```

## 方案3：**使用分布式锁控制写入（如 Redis + Redisson）**

```java
String lockKey = "upload:" + fileName;
try {
    RLock lock = redissonClient.getLock(lockKey);
    if (lock.tryLock(3, 10, TimeUnit.SECONDS)) {
        // 写文件逻辑
    }
} finally {
    lock.unlock();
}
```



# 大文件上传， 如何保证效率和高可用？

这是一个非常经典的高并发系统设计题，考察你对**大文件上传的分片（分块）机制、断点续传、并发控制、存储设计、高可用容错机制**的综合能力。



## 问题本质分析

大文件上传的难点在于：

- 文件体积大（上传耗时长）
- 网络不稳定（容易中断）
- 多用户并发（资源竞争大）
- 服务异常（需容错/续传）



## 关键设计点详解

### 架构

```text
前端：大文件切片
  ↓
后端：接收分片 + 校验 + 临时存储
  ↓
数据库/Redis：记录上传进度、文件哈希
  ↓
最终合并 + 持久化到分布式存储（如 OSS）
```

### **1. 分片上传（Chunk Upload）**

**前端将大文件拆成多个小分片并并行上传**

- 减少单次上传压力，提高上传成功率
- 每个分片单独处理，失败可重传



### **2. 上传唯一标识（秒传优化）**

**文件内容做哈希（如 MD5、SHA256）生成唯一 ID**

- 秒传优化：服务端查 MD5，有则直接返回上传成功
- 防重复上传、做断点续传定位用



### **3.断点续传 + 幂等控制**

**服务端记录每个用户上传的文件分片状态**

```
Key: upload:md5:{userId}
Value: bitmap 或已上传分片列表（如 1,2,3,5）
```

- 断点续传时，前端读取未上传片段继续发
- 幂等：同一分片多次上传结果一致，服务端判断已存在则跳过



### **4. 合并分片 + 校验完整性**

- 所有分片上传完成后，触发合并操作（后端合并或由对象存储提供合并接口）
- 最后校验 MD5 与原始文件一致，确保无误



### **5. 高可用存储设计**

上传后的数据要保证可靠：

- 使用对象存储（如阿里 OSS、S3、MinIO），天然支持分片、断点续传
- 存储服务本身支持多副本、高可用、异地灾备
- 元数据存数据库 + 异步备份到冷存储或消息队列做异步处理



## 操作系统相关优化

### **1. 零拷贝（Zero-copy）**

传统上传流程会产生多次用户态/内核态的**上下文切换和数据拷贝**：

```
硬盘 → 内核缓冲区 → 用户态缓冲区 → Socket缓冲区 → 网卡
```

![image-20250413160526696](./场景题总结.assets/image-20250413160526696.png)

可以使用 **sendfile+DMA拷贝** 等方式实现 **零拷贝**，减少 CPU 拷贝负担，提升 I/O 吞吐。

- 它可以替代前⾯的 read() + write() 这两个系统调⽤
  - 数据从磁盘到网卡，CPU 不再拷贝，减少 2 次上下文切换
  - 数据直接从 Page Cache 传输到网卡，完全避免 CPU 参与拷贝
- 2次上下文切换 和 0次数据拷贝



FileChannel.transferTo()实现零拷贝

```java
public void sendFile(Socket socket, File file) throws IOException {
    try (FileInputStream fis = new FileInputStream(file);
         FileChannel fileChannel = fis.getChannel();
         OutputStream out = socket.getOutputStream()) {

        WritableByteChannel target = Channels.newChannel(out);
        fileChannel.transferTo(0, file.length(), target);
    }
}
```



### **2. 多线程并发上传（IO 多路复用）**

使用线程池或异步 IO（如 NIO、Netty）来同时上传多个分片：

- 利用多核 CPU 并行处理
- 不阻塞主线程，提升吞吐
- **异步**写盘防止磁盘 I/O 成为瓶颈，MQ



## 计算机网络相关优化

### **1. 并发连接（分片 + 并发）**

- HTTP 是基于 TCP 的，单连接上限通常在 10~50 Mbps
- 使用多个连接（多线程/分片）并发上传，达到**聚合带宽效果**
- 实测：将 1GB 文件拆为 20 片并发上传，时间缩短近 70%



# 订单结算场景， 如何设计高可用且高并发的服务？



## **问题定义：电商订单结算的核心挑战**

> 订单结算 = 用户下单 → 商品库存校验 → 优惠券抵扣 → 运费计算 → 总价生成 → 锁定资源（库存/优惠）→ 返回给用户确认页

**典型难点：**

- 高并发下系统容易崩溃
- 多个依赖服务（库存、优惠券、运费）可能不稳定
- 数据要强一致，不能多扣库存、重复使用优惠券
- 用户体验要快（页面秒出）



## **核心模块设计方案**



### **前端防抖 + 请求幂等**

- 下单按钮加前端防抖（1秒内不能点两次）
- 后端用分布式锁做幂等注解，防止重复提交



### **API 网关层：限流 + 鉴权 + 灰度**

- 限流（如 Sentinel、Nginx 限速）：防止恶意刷单
- 鉴权：保护接口调用
- 熔断/降级：防止服务雪崩扩散



### **服务拆分（按职责解耦）**

拆分有利于**单服务故障不影响全局**

- **结算服务**：只负责计算总价，调用商品、库存、优惠券等服务
- **订单服务**：最终下单入库
- **库存服务**：锁库存、扣库存
- **优惠券服务**：校验 + 锁定优惠券



### **资源锁定机制（避免超卖/重复使用）**

在订单创建之前，需锁定资源：

#### **库存锁定：**

- Redis 预扣库存（扣减时原子 Lua 脚本）
- 锁失败即提示“库存不足”

#### **优惠券锁定：**

- 使用 分布式锁 加锁
- 成功则标记该券已被占用（等待支付）



### **服务间通信：异步化提升性能**

- 使用 RocketMQ / Kafka 异步处理：

  - 日志记录（下单日志、行为日志）
  - 异步通知（短信、积分发放）
  - 支付成功 → 通知库存服务 → 实际扣减

  

### **分布式事务处理方案**

为了保证 **结算→下单→扣库存** 的一致性：

RocketMQ 事务消息（先发送 half 消息，等本地事务完成后 commit）





# 如何设计一个IM系统

这是一道**系统设计 + 网络协议栈 + 实时通信机制**综合考察题，面试官希望你能从 **“计算机网络角度”**切入，而不是仅谈业务逻辑或数据库设计



## **IM系统的本质（从网络视角理解）**

**即时通讯系统**的核心目标是：

- 保证消息的**实时性**
- 支持**双向通信（全双工）**
- 网络连接要**稳定、低延迟、低丢包**

从网络协议角度看，本质上是一个**长连接（长生命周期）+ 异步收发 + 多路复用的网络服务系统**。



## **传输协议选择：TCP vs UDP vs QUIC**

| **协议** | **是否有连接** | **可靠性** | **适用性**                     |
| -------- | -------------- | ---------- | ------------------------------ |
| TCP      | 是             | ✔️可靠      | 主流选择，支持长连接           |
| UDP      | 否             | ❌不可靠    | 需要自定义可靠机制，适合音视频 |
| QUIC     | 是（基于UDP）  | ✔️可靠      | 新协议，连接迁移能力强         |



**传统IM系统通常选择： TCP + 自定义协议**， 因为

- 可靠传输（不丢消息、不乱序）
- 长连接支持
- 可配合心跳包、滑动窗口、ACK机制
- TCP是全双工的

但**Quick**协议在 IM 场景中具备明显优势



## **QUIC 相比 TCP 的核心优势**

**零 RTT 建立连接（0-RTT）**

- TCP 建立连接需要 3 次握手（3-RTT 才能完成 TLS + 应用层）
- QUIC **1 次握手甚至 0-RTT** 就能发送应用数据（类似缓存 TLS 会话）

在移动端 IM 场景中，**每次重连能快几十～上百毫秒**



**多路复用无队头阻塞**

- TCP 是按字节流传输，一个包丢了 → 整个连接阻塞等待重传
- QUIC 内建**多路复用**机制，**丢一个 stream 不会影响其他 stream**

IM 中：比如文字消息和文件同时传，文件丢了包不会影响文字显示速度。

**总结**

> QUIC 在 IM 场景中具备明显优势：它基于 UDP 实现，自带多路复用机制，能避免 TCP 的队头阻塞问题；同时内建 TLS 1.3 加密，大幅提升安全性和连接建立速度。特别适合移动场景，它支持连接迁移，允许设备从 WiFi 切换到移动网络后不重连，大大提升用户体验。QUIC 的握手速度远优于 TCP + TLS 的三次握手，支持 0-RTT 数据传输，是未来实时通信的主流趋势。



## **连接模型设计（客户端与服务端）**

**单客户端长连接**

客户端和服务端之间维持一条 **TCP 长连接**，避免频繁三次握手/挥手。



服务端使用**高并发连接模型**

- Reactor 多路复用（如 **Netty** 使用的 NIO）
- 每个连接使用线程池处理 IO 事件
- 使用**非阻塞 IO（NIO）**或 **EPOLL** 提高性能



## **心跳 + 空闲连接检测**

为了防止**连接假死**，服务端要定期检测客户端是否活着：

- 客户端每隔 30s 发送 PING（心跳包）
- 服务端响应 PONG
- 若超过一定时间未收到心跳，则**断开连接释放资源**



## **消息可靠传输机制（网络传输层上补强）**

虽然 TCP 保证可靠性，但应用层还需处理：

| **问题**           | **应对策略**                     |
| ------------------ | -------------------------------- |
| 客户端消息发送失败 | 本地重试 + 本地消息队列          |
| 服务端转发失败     | 推送失败消息入队列（MQ、DB）     |
| 消息重复投递       | 消息ID做幂等控制                 |
| 网络异常断线重连   | 客户端自动重连 +离线消息补偿机制 |



# 如何设计动态线程池

动态线程池中，怎么监控这个线程池的状态？怎么知道什么时候该给它加点线程了？或者说什么时候你是需要扩容机器的？



## **需要设置哪些“监控指标”**

**系统层面指标（线程池维度）**

| **指标**                          | **含义**           | **建议阈值设置**        |
| --------------------------------- | ------------------ | ----------------------- |
| activeCount                       | 当前活跃线程数     | ≥ maximumPoolSize × 0.9 |
| queue.size()                      | 当前等待队列长度   | ≥ queueCapacity × 0.8   |
| taskCount vs completedTaskCount   | 任务积压量         | 持续差距变大说明阻塞    |
| threadPoolQueueWaitTime（自定义） | 平均排队等待时间   | > 500ms 就是“卡顿”预警  |
| 拒绝任务数（rejected）            | 被拒绝执行的任务数 | ≥ 1 立即报警            |



**业务层面指标（结合业务语义）**

| **指标**                | **含义**                        | **建议阈值设置**       |
| ----------------------- | ------------------------------- | ---------------------- |
| 请求耗时（avg latency） | 接口平均响应时间                | > SLA（如 > 500ms）    |
| 成功率/错误率           | 5xx 错误率、超时率              | > 5% 报警              |
| QPS / TPS               | 当前业务的吞吐量                | 可配历史均值 ± 偏移量  |
| 请求分布 / 热点分析     | 某些 case 的 QPS 明明很低但很慢 | 单独报警               |
| 接口等级（SLA等级）     | 比如“支付” vs “搜索建议”        | 不同等级接口设不同阈值 |



**为什么不能只看系统指标？**

不要只看线程池队列/活跃线程数，还要看业务表现，

| **线程池状态**              | **QPS** | **延迟** | **业务表现**       | **判断结果**       |
| --------------------------- | ------- | -------- | ------------------ | ------------------ |
| 活跃线程数低                | 5       | 1500ms   | 响应慢             | 可能 DB 卡住       |
| 活跃线程数满                | 500     | 100ms    | 响应快             | 没必要扩容         |
| 队列积压多                  | 50      | 800ms    | 仅发生在非核心接口 | 可容忍或降级处理   |
| 活跃线程高 + 业务成功率下降 | 200     | 1200ms   | 用户投诉多         | 需要扩容或优化逻辑 |



## **如何设定阈值？（实战经验）**

**分接口设阈值**

- 核心接口（如下单、支付）更敏感：队列阈值可设低些（80%就报警）
- 弱一致性接口（日志、推荐）容忍度高



## **怎么判断当前线程数/机器资源“不够用了”，需要扩容或调优**

### **【加线程】的常见信号：**

| **现象**                             | **原因**               | **应对**                   |
| ------------------------------------ | ---------------------- | -------------------------- |
| ActiveCount 经常等于 maximumPoolSize | 所有线程满负载工作中   | 说明线程数可能不够         |
| Queue.size() 持续变大                | 任务来得快，处理不过来 | 可考虑加线程或改用拒绝策略 |
| 任务平均等待时间变长                 | 队列阻塞明显           | 增加 corePoolSize          |
| 任务执行时间短 + 队列爆炸            | 高吞吐任务             | 增加线程池并发度更有效     |



### **【扩机器】的信号：**

| **现象**                                     | **原因**         | **应对**               |
| -------------------------------------------- | ---------------- | ---------------------- |
| 已经把线程池配得很大，但 CPU 使用率长期 80%+ | CPU 成为瓶颈     | **扩机器**（水平扩展） |
| GC频繁、Full GC 增多                         | 内存压力过大     | 扩内存或增加节点分摊   |
| 网络 I/O 拖慢线程吞吐                        | 网络资源不够     | 拆分节点 / 服务分离    |
| 单台 QPS 接近极限                            | 系统承压接近上限 | 扩实例 + 负载均衡      |





# 事务消息实现：用户支付订单+下游服务

## 事务消息原理

![事务消息](https://help-static-aliyun-doc.aliyuncs.com/assets/img/zh-CN/0105054561/p446186.png)

```
生产者将消息发送至云消息队列 RocketMQ 版服务端。

云消息队列 RocketMQ 版服务端将消息持久化成功之后，向生产者返回Ack确认消息已经发送成功，此时消息被标记为“暂不能投递”，这种状态下的消息即为半事务消息。

生产者开始执行本地事务逻辑。

生产者根据本地事务执行结果向服务端提交二次确认结果（Commit或是Rollback），服务端收到确认结果后处理逻辑如下：

二次确认结果为Commit：服务端将半事务消息标记为可投递，并投递给消费者。

二次确认结果为Rollback：服务端将回滚事务，不会将半事务消息投递给消费者。

在断网或者是生产者应用重启的特殊情况下，若服务端未收到发送者提交的二次确认结果，或服务端收到的二次确认结果为Unknown未知状态，经过固定时间后，服务端将对消息生产者即生产者集群中任一生产者实例发起消息回查。
```



## 实际业务实例

> 目标：用户支付订单成功后，需要更新多个业务系统（订单系统、物流、积分、购物车等），这些操作要么**全部成功**，要么**全部不发生**，即保持一致性。

![事务消息](https://help-static-aliyun-doc.aliyuncs.com/assets/img/zh-CN/4762246561/p446983.png)

执行流程图

```
用户请求下单
     ↓
sendMessageInTransaction("扣库存消息")
     ↓
↓Prepare消息存入MQ（消费者不可见）
↓
回调本地 executeLocalTransaction() → 创建订单写库
     ↓
写库成功         写库失败
↓                    ↓
COMMIT        ROLLBACK
消息可见        消息丢弃
↓
库存系统消费者 → 扣减库存

如果生产者崩溃，RocketMQ 会触发 checkLocalTransaction() 回查
```



## 生产者代码

```java
// 发送事务消息
SendResult result = producer.sendMessageInTransaction(msg, arg);

// 实现事务监听器
TransactionListener listener = new TransactionListener() {
    // 执行本地事务（更新订单）
    public LocalTransactionState executeLocalTransaction(Message msg, Object arg) {
        boolean ok = orderService.updateOrderStatus(msg.getKey());
        return ok ? COMMIT_MESSAGE : ROLLBACK_MESSAGE;
    }

    // 回查事务状态（RocketMQ 检查未提交事务时调用）
    public LocalTransactionState checkLocalTransaction(Message msg) {
        return orderService.isUpdated(msg.getKey()) ? COMMIT_MESSAGE : ROLLBACK_MESSAGE;
    }
};
```

总结

> **RocketMQ 事务消息是通过“发送半消息 + 回调本地事务 + 状态回查”机制，实现分布式操作与消息发送之间的强一致性保障。**





# 百万点赞，如何设计点赞系统

### 数据库表设计：

**1. 点赞记录表（likes）**

- **作用**：记录每一次点赞事件，支持溯源、查询、去重。
- **核心字段**：
  - `Mid`（用户 ID）：点赞用户。
  - `messageID`（实体 ID）：被点赞的对象（如文章、评论、视频等）。
  - **其他字段**：
    - **点赞来源**（可能是 App、Web、小程序等）。
    - **点赞时间**。
  - **索引**：
    - `Mid, messageID` 联合索引，**优化查询是否点赞**。

**2. 点赞统计表（counts）**

- **作用**：维护每个被点赞实体的总点赞数、点踩数等，减少 `COUNT` 查询开销。
- **主键**：
  - `BusinessID`（业务 ID）：区分不同业务场景（如视频、文章）。
  - `messageID`（实体 ID）：被点赞的具体对象。
- **其他字段**：
  - `like_count`（点赞数）。
  - `dislike_count`（点踩数）。
- **索引**：
  - `messageID` 维度索引，优化业务查询。

### redis缓存设计

- 存储点赞状态：

  - **作用**：判断用户是否点过赞，避免查数据库。

  - **数据结构**：使用 **Redis `Set`** 存储点赞用户 ID。

- 存储点赞计数
  - **作用**：减少 `COUNT(*)` 查询，直接返回点赞数。
  - **数据结构**：使用 **Redis `String`** 存储点赞数。

### RocketMQ 异步写入数据库

可以用**批量消费** RocketMQ进行优化：在写入点赞数和点赞事件的时候，可以累计10s内的点赞数和点赞时间，然后一次性批量写入数据库，可以减少数据库的IO次数

### 定期同步 Redis 和数据库，可以用定时任务

为了解决

Redis 宕机，数据丢失；异步写入导致数据库延迟



# 集群模式下，N个节点，写入时写W个节点，读的时候读R个节点，满足什么条件能保证读到最新数据？

**N**：集群中的总副本数（节点数）。

**W**：一次写操作必须更新的副本数量。

**R**：一次读操作必须从多少个副本读取数据。

### **(1) 强一致性（Strong Consistency）**

- **条件**：`W + R > N`

意味着：

- **写入至少覆盖 W 个副本**。
- **读取时至少访问 R 个副本**
- 保证 **写入和读取的交集至少有 1 个副本** 持有最新数据。

### **(2) 最终一致性（Eventual Consistency）**

- **条件**：`W + R <= N`
- **特点**：
  - 允许数据短时间内不一致，但最终同步。
  - 适用于 **社交点赞、推荐系统、缓存等高并发场景**。

### **(3) 读后写一致性（Read-Your-Writes Consistency）**

- **条件**：`W > N/2 且 R = 1`
- **特点**：
  - **用户自己修改的数据，自己一定能读到最新值**。
  - 适用于 **用户个性化数据，如购物车、个人资料修改等**。



# 秒杀架构

### 秒杀场景

![image-20250327140801198](./场景题总结.assets/image-20250327140801198.png)

### 秒杀要考虑的点

![image-20250327140816408](./场景题总结.assets/image-20250327140816408.png)

![image-20250327141133671](./场景题总结.assets/image-20250327141133671.png)

### 高并发响应

![image-20250327145319757](./场景题总结.assets/image-20250327145319757.png)

![image-20250327141350727](./场景题总结.assets/image-20250327141350727.png)

### 防止超卖

![image-20250327141429398](./场景题总结.assets/image-20250327141429398.png)

### 解决少卖

![image-20250327145641427](./场景题总结.assets/image-20250327145641427.png)

### 用户限流，防止刷子

![image-20250327141456096](./场景题总结.assets/image-20250327141456096.png)

![image-20250327142128525](./场景题总结.assets/image-20250327142128525.png)

![image-20250327142238281](./场景题总结.assets/image-20250327142238281.png)

![image-20250327142437981](./场景题总结.assets/image-20250327142437981.png)

### 异步处理订单

![image-20250327142535851](./场景题总结.assets/image-20250327142535851.png)

### 订单失败补偿

![image-20250327142630918](./场景题总结.assets/image-20250327142630918.png)

### 服务降级

![image-20250327142748450](./场景题总结.assets/image-20250327142748450.png)

### 缓存数据库的一致性

- Redis 库存为空后，发送个延时消息比如 5 秒后请求数据库查看是否还有库存，有的话刷到缓存里。
- 定时任务，定期将数据库的数据同步到缓存中

# 两个50亿的url找出相同url

![image-20250327143018819](./场景题总结.assets/image-20250327143018819.png)

### 方案1

![image-20250327143041957](./场景题总结.assets/image-20250327143041957.png)

![image-20250327143201931](./场景题总结.assets/image-20250327143201931.png)



### 方案2

![image-20250327143317987](./场景题总结.assets/image-20250327143317987.png)

![image-20250327143404417](./场景题总结.assets/image-20250327143404417.png)



# 从1000w记录中，找出最热门的10个记录？

![image-20250327150530429](./场景题总结.assets/image-20250327150530429.png)

![image-20250327151100479](./场景题总结.assets/image-20250327151100479.png)

### 方案

![image-20250327151137840](./场景题总结.assets/image-20250327151137840.png)



# 在一亿个数中找出最大的1万个数

![image-20250327152702515](./场景题总结.assets/image-20250327152702515.png)

![image-20250327152738059](./场景题总结.assets/image-20250327152738059.png)



# 设计商品排行榜





# 十万级发券业务性能瓶颈及解决方案



# 下了订单后，可能会有库存模块，可能有配送模块，消息队列一个消息发出有多个模块收到，你觉得怎么设计好一点？



# 如何实现跨域单点登录（如淘宝登录后访问天猫自动登录）？

### 核心思路：

```
跨域 SSO 的关键是浏览器不能共享 Cookie，所以各子系统通过重定向到统一 SSO 中心来确认登录状态。
登录成功后，SSO 在自己的域下保存登录态，然后通过一次性授权码（code）重定向回子系统，子系统后端拿 code 向 SSO 获取用户信息，建立本地会话。
这样就实现了登录一次，多个系统自动登录的能力。
```

### 场景设定

```
天猫和淘宝不是一个域名，比如：

	•	淘宝：taobao.com
	•	天猫：tmall.com

跨域意味着：两个系统不能共享 Cookie（同源策略）
```

### 实现

```
🎯 核心组件：
	•	✅ 用户（浏览器端）
	•	✅ 淘宝系统（子系统 A）
	•	✅ 天猫系统（子系统 B）
	•	✅ 统一认证中心（SSO Server）

⸻

✅ 登录流程分步解析：

✅ 1）未登录时访问淘宝
	•	用户访问 taobao.com，发现本地没有登录态
	•	浏览器被 302 重定向到 SSO 登录中心：sso.com/login?redirect=taobao.com

✅ 2）SSO 登录中心处理登录
	•	用户输入账号密码，登录成功后：
	•	在 sso.com 域下，写入登录 Cookie（如 SSO-TOKEN）
	•	生成一个一次性授权码（如 code=xyz123）
	•	重定向回淘宝：taobao.com?code=xyz123

✅ 3）淘宝收到授权码，向 SSO 请求用户信息
	•	淘宝通过后端请求：GET sso.com/token/validate?code=xyz123
	•	SSO 校验成功，返回用户信息
	•	淘宝给用户发本地 Cookie（例如 JWT 或 Session）

✅ 此时用户在淘宝就“登录”了，但SSO Cookie 还保留在 sso.com 域下

⸻

✅ 实现“天猫自动登录”

✅ 用户访问天猫时
	•	天猫也发现没有登录态
	•	天猫重定向到 sso.com/login?redirect=tmall.com

✅ 因为浏览器访问的是 sso.com：
	•	浏览器带上了之前写入的 Cookie（SSO-TOKEN）
	•	SSO 发现用户已经登录 ✅
	•	立即生成新的一次性授权码，重定向回天猫：tmall.com?code=abc456

✅ 天猫根据 code 再次调用 SSO 获取用户信息
	•	自己创建本地 Cookie，完成自动登录

⸻

✅ 跨域如何传递登录状态？
	•	浏览器不能共享 taobao.com 和 tmall.com 的 Cookie
	•	但 两个系统都能访问同一个 sso.com 域
	•	所以关键就是：让所有系统通过跳转 + code 来“间接共享登录状态”

```



# 50万个坐标点，如何设计数据结构来找到距离一个点最近的10个点

**KD-Tree + KNN 查询（最常用）**

###  **1. 数据结构：KD-Tree**

构建过程：

​	1.	第 0 层用 x 维排序，选择中位数为根

​	2.	左边构建左子树，右边构建右子树

​	3.	第 1 层用 y 维排序（交替）

	4.	递归下去直到叶子节点

复杂度：**O(n log n)**（平衡二叉树）



### **2. 查询算法：K 近邻搜索（KNN）**

**✅ 第一步：递归下探，找到目标所在叶子节点**

​	•	类似二叉查找，按分割维度比较大小

​	•	找到离目标点最近的“子空间区域”



------



**✅ 第二步：回溯 + 剪枝 + 小顶堆维护最近 K 个点**

​	•	回溯过程中计算当前节点与目标点距离

​	•	用**小顶堆（最大长度 K）**维护当前最近的 K 个点

​	•	判断另一个子空间是否有可能存在更近的点（即圆与分割线交叉）

​	•	若可能 ➜ 继续递归搜索另一个分支



------



**✅ 查询复杂度：**

​	•	平均情况：**O(log n + k log k)**
